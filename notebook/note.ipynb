{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Data Extraction from MarketAUX API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Library\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# BigQuery\n",
    "from google.oauth2 import service_account\n",
    "from pandas_gbq import to_gbq\n",
    "\n",
    "# Logging\n",
    "import logging\n",
    "# Setup logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Dotenv\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL without page param\n",
    "BASE_URL = \"https://api.marketaux.com/v1/news/all\"\n",
    "API_TOKEN = \"NEqVFjBNHIqWoYCYmABZTcDpfWhL5tIva3TmLLsc\"\n",
    "\n",
    "# Parameters assignment\n",
    "published_after_date = (datetime.utcnow() - timedelta(days=90)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Parameters (token + language stays the same, page varies)\n",
    "params = {\n",
    "    \"language\": \"en,id\",\n",
    "    \"must_have_entities\": \"true\",\n",
    "    \"published_after\": published_after_date,\n",
    "    \"api_token\": API_TOKEN\n",
    "}\n",
    "\n",
    "# Storage for all articles\n",
    "all_articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 1 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 2 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 2 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 3 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 3 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 4 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 4 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 5 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 5 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 6 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 6 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 7 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 7 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 8 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 8 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 9 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 9 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 10 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 10 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 11 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 11 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 12 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 12 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 13 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 13 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 14 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 14 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 15 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 15 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 16 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 16 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 17 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 17 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 18 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 18 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 19 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 19 | Status code: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fetched page 20 successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 20 | Status code: 200\n"
     ]
    }
   ],
   "source": [
    "# Loop over 20 pages (pagination approach due to limit per day)\n",
    "for page in range(1, 21):  # 1 to 20\n",
    "    params[\"page\"] = page\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    print(f\"Page {page} | Status code: {response.status_code}\")\n",
    "    logging.info(f\"Fetched page {page} successfully.\")\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed on page {page}\")\n",
    "        continue\n",
    "\n",
    "    result = response.json()\n",
    "    page_articles = result.get(\"data\", [])\n",
    "\n",
    "    all_articles.extend(page_articles)\n",
    "\n",
    "    # Optional delay to avoid being flagged as abusive (depends on API rules)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Transform Data into Readily Available Use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialze the list holder for selected data\n",
    "article_collection = []\n",
    "\n",
    "# Select each columns for the new data holder\n",
    "for article in all_articles:\n",
    "    symbol = \"unknown\"\n",
    "    name = \"unknown\"\n",
    "    type = \"undetermined\"\n",
    "    industry = \"undetermined\"\n",
    "\n",
    "    if article.get(\"entities\"):\n",
    "        entity = article[\"entities\"][0]\n",
    "        symbol = entity.get(\"symbol\", \"unknown\")\n",
    "        name = entity.get(\"name\", \"unknown\")\n",
    "        type = entity.get(\"type\", \"undetermined\")\n",
    "        industry = entity.get(\"industry\", \"undetermined\")\n",
    "\n",
    "    article_collection.append({\n",
    "        \"title\": article.get(\"title\", \"\"),\n",
    "        \"keywords\": article.get(\"keywords\", \"\"),\n",
    "        \"url\": article.get(\"url\", \"\"),\n",
    "        \"image_url\": article.get(\"image_url\", \"\"),\n",
    "        \"published_at\": article.get(\"published_at\", \"\"),\n",
    "        \"source\": article.get(\"source\", \"\"),\n",
    "        \"language\": article.get(\"language\", \"\"),\n",
    "        \"symbol\": symbol,\n",
    "        \"name\": name,\n",
    "        \"type\": type,\n",
    "        \"industry\": industry\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the article_collection to a DataFrame (optional)\n",
    "df = pd.DataFrame(article_collection)\n",
    "\n",
    "# Enriching and deleting keywords column\n",
    "df[\"list_of_keywords\"] = df[\"keywords\"].str.split(\", \")\n",
    "df.drop(columns=[\"keywords\"], inplace=True)\n",
    "\n",
    "# Changing data types for better suitability\n",
    "df[\"title\"] = df[\"title\"].astype(str)\n",
    "df[\"url\"] = df[\"url\"].astype(str)\n",
    "df[\"image_url\"] = df[\"image_url\"].astype(str)\n",
    "df[\"published_at\"] = pd.to_datetime(df[\"published_at\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "df[\"source\"] = df[\"source\"].astype(str)\n",
    "df[\"language\"] = df[\"language\"].astype(str)\n",
    "df[\"symbol\"] = df[\"symbol\"].astype(str)\n",
    "df[\"name\"] = df[\"name\"].astype(str)\n",
    "df[\"type\"] = df[\"type\"].astype(\"category\")\n",
    "df[\"industry\"] = df[\"industry\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. Save data into CSV\n",
    "df.to_csv(f\"../data/econNews-{datetime.now().strftime('%Y%m%d')}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Loading into BigQuery**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_bigquery(\n",
    "    df: pd.DataFrame,\n",
    "    table_id: str,\n",
    "    project_id: str,\n",
    "    credentials_path: str,\n",
    "    if_exists: str = \"append\"):\n",
    "    \"\"\"\n",
    "    Upload a pandas DataFrame to Google BigQuery using service account credentials.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Cleaned and enriched data to upload.\n",
    "        table_id (str): Full table path in BigQuery (e.g., dataset.table_name).\n",
    "        project_id (str): Google Cloud project ID.\n",
    "        credentials_path (str): Path to the service account JSON key file.\n",
    "        if_exists (str): What to do if table exists. Options: 'fail', 'replace', 'append'.\n",
    "                         Default is 'append'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Authenticating using service account at: {credentials_path}\")\n",
    "        credentials = service_account.Credentials.from_service_account_file(\n",
    "            credentials_path\n",
    "        )\n",
    "\n",
    "        logging.info(f\"Uploading to BigQuery table: {table_id} (mode: {if_exists})\")\n",
    "        to_gbq(\n",
    "            dataframe=df,\n",
    "            destination_table=table_id,\n",
    "            project_id=project_id,\n",
    "            if_exists=if_exists,\n",
    "            credentials=credentials\n",
    "        )\n",
    "        logging.info(\"Upload completed successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to upload to BigQuery: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigQuery configurations\n",
    "PROJECT_ID = \"econnews-daily-update \"\n",
    "TABLE_ID = \"econnews_daily\"\n",
    "CREDENTIALS_PATH = \"../config/econnews-daily-update-2e6ea15bf774.json\"\n",
    "\n",
    "upload_to_bigquery(\n",
    "        df=df,\n",
    "        table_id=TABLE_ID,\n",
    "        project_id=PROJECT_ID,\n",
    "        credentials_path=CREDENTIALS_PATH,\n",
    "        if_exists=\"append\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
